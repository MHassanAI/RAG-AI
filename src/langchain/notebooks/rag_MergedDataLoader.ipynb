{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `MergedDataLoader`: Retrieval Augmented Generation\n",
    "\n",
    "## Synthesize Answers to Coding Questions over Multiple Repositories of Code, & Web Docs\n",
    "\n",
    "### Learning Objectives:\n",
    "*Learn how to load data from multiple sources and in multiple ways before retrieving their context for retrieval augmented generation.*\n",
    "\n",
    "Retrieval augmented generation is when we improve the quality of LLM-generated answers by using domain-specific knowledge.\n",
    "\n",
    "**Goals**\n",
    "1) Use `GenericLoader` and `LanguageParser` to load Python files from GitHub repositories.\n",
    "2) Load all documents into a single vectorstore via `MergedDataLoader`.\n",
    "3) Use a vectorstore as a retriever to index multiple repositories and webpage documentation for retrieval augmented generation.\n",
    "4) Answer user's coding questions using synthesized knowledge from documentation for Streamlit, LangGraph and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langchain-community langchain-core GitPython langchain-openai beautifulsoup4 faiss-cpu langchain-anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = \"YOUR_API_KEY\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"MultiSource-RAG-MergedDataLoader\"\n",
    "os.environ[\"openai_api_key\"] = \"YOUR_API_KEY\"\n",
    "os.environ[\"anthropic_api_key\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading Repositories using the Git Python library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### GIT REPO DOWNLOADING ###\n",
    "from git import Repo\n",
    "\n",
    "# Clone\n",
    "langchain_path = \"./langchain-library\"  # directory to clone the repository to \\\n",
    "# `./` will create a sub-directory in the current working directory, named langchain-library\n",
    "\n",
    "# Clone the repo if the sub-directory above doesn't exist\n",
    "if not os.path.exists(langchain_path):\n",
    "    repo = Repo.clone_from(\n",
    "        \"https://github.com/langchain-ai/langchain\", to_path=langchain_path\n",
    "    )\n",
    "\n",
    "langgraph_path = \"./langgraph-library\"\n",
    "\n",
    "# Clone the repo if the sub-directory above doesn't exist\n",
    "if not os.path.exists(langgraph_path):\n",
    "    repo = Repo.clone_from(\n",
    "        \"https://github.com/langchain-ai/langgraph\", to_path=langgraph_path\n",
    "    )\n",
    "\n",
    "streamlit_path = \"./streamlit-library\"\n",
    "\n",
    "# Clone the repo if the sub-directory above doesn't exist\n",
    "if not os.path.exists(streamlit_path):\n",
    "    repo = Repo.clone_from(\n",
    "        \"https://github.com/streamlit/streamlit\", to_path=streamlit_path\n",
    "    )\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specifically load Python files from each repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### REPOSITORY LOADING ###\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import Language\n",
    "\n",
    "# Use `GenericLoader` to load Python files from the cloned repository\n",
    "load_langchain_api_docs = GenericLoader.from_filesystem(\n",
    "    langchain_path + \"/libs/langchain/langchain\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],  # Specify a list of file types\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "\n",
    "load_langgraph_api_docs = GenericLoader.from_filesystem(\n",
    "    langgraph_path + \"/langgraph\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],  # Specify a list of file types\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "\n",
    "load_streamlit_api_docs = GenericLoader.from_filesystem(\n",
    "    streamlit_path + \"/lib/streamlit\",\n",
    "    glob=\"**/*\",\n",
    "    suffixes=[\".py\"],  # Specify a list of file types\n",
    "    parser=LanguageParser(language=Language.PYTHON, parser_threshold=500),\n",
    ")\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape web documentation for Streamlit and LangChain\n",
    "\n",
    "Here we use a `max_depth` of '5' to try and be thorough as possible without going too deep.\n",
    "\n",
    "*Refer to the documentation you're going to scrape before configuring that parameter.*\n",
    "\n",
    "You'll know how many steps to set it by based on how many path segments are in the URL.\n",
    "> For example: \n",
    ">> Say you're going to scrape `https://docs.streamlit.io/library/api-reference/charts/st.area_chart` and your base URL is `https://docs.streamlit.io/`\n",
    ">>> Your `max_depth` must be at least 5. This is because there are 5 path segments in the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WEB LOADING ###\n",
    "from langchain_community.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from bs4 import BeautifulSoup as Soup\n",
    "\n",
    "streamlit_url = \"https://docs.streamlit.io/library/api-reference/\"\n",
    "streamlit_loader = RecursiveUrlLoader(\n",
    "    url=streamlit_url, max_depth=5, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "\n",
    "langchain_url = \"https://python.langchain.com/docs/\"\n",
    "langchain_loader = RecursiveUrlLoader(\n",
    "    url=langchain_url, max_depth=5, extractor=lambda x: Soup(x, \"html.parser\").text\n",
    ")\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Preparation for Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### LOAD EVERYTHING ###\n",
    "from langchain_community.document_loaders.merge import MergedDataLoader\n",
    "\n",
    "documents = MergedDataLoader(\n",
    "    loaders=[\n",
    "        load_langchain_api_docs,\n",
    "        load_langgraph_api_docs,\n",
    "        load_streamlit_api_docs,\n",
    "        streamlit_loader,\n",
    "        langchain_loader,\n",
    "    ]\n",
    ")\n",
    "\n",
    "all_docs = documents.load()\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You have 2422 document(s) in your data\n",
      "\n",
      "There are 217 characters in your sample document\n",
      "\n",
      "Here is a sample: \n",
      "\n",
      "```\n",
      "\n",
      "\"\"\"Deprecated module for BaseLanguageModel class, kept for backwards compatibility.\"\"\"\n",
      "from __future__ import annotations\n",
      "\n",
      "from langchain_core.language_models import BaseLanguageModel\n",
      "\n",
      "__all__ = [\"Bas\n",
      "\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "# Note: If you're using PyPDFLoader then it will split by page for you already\n",
    "print(f\"\\nYou have {len(all_docs)} document(s) in your data\")\n",
    "print(f\"\\nThere are {len(all_docs[0].page_content)} characters in your sample document\")\n",
    "print(f\"\\nHere is a sample: \\n\\n```\\n\\n{all_docs[0].page_content[:200]}\\n\\n```\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEXT SPLITTING ###\n",
    "\n",
    "# Instantiate a text splitter\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500, chunk_overlap=50, length_function=len\n",
    ")\n",
    "\n",
    "# Split the documents\n",
    "processed_documents = text_splitter.split_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an embeddings models\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    dimensions=256,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CREATE A VECTORSTORE AS A RETRIEVER ###\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Check for the local index's existence\n",
    "if os.path.exists(\"./codegen_faiss\"):\n",
    "\n",
    "    # Load the local index\n",
    "    vectorstore = FAISS.load_local(\n",
    "        folder_path=\"./codegen_faiss\",\n",
    "        embeddings=embeddings,\n",
    "        allow_dangerous_deserialization=True, # Not recommended for production\n",
    "    )\n",
    "    \n",
    "    retriever = vectorstore.as_retriever(\n",
    "        # Optionally configure retrieval parameters\n",
    "        # search_type=\"mmr\", search_kwargs={\"k\": 7, \"fetch_k\": 14}\n",
    "    )\n",
    "\n",
    "else:\n",
    "    \n",
    "    # Embed documents in a vectorstore\n",
    "    vectorstore = FAISS.from_documents(\n",
    "        processed_documents,\n",
    "        embeddings,\n",
    "    )\n",
    "\n",
    "    # Save the vectorstore locally\n",
    "    vectorstore.save_local(\n",
    "        folder_path=\"./codegen_faiss\",\n",
    "    )\n",
    "\n",
    "    # Configure the retriever\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        # Optionally configure retrieval parameters\n",
    "        # search_type=\"mmr\", search_kwargs={\"k\": 7, \"fetch_k\": 14}\n",
    "    )\n",
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now that we have a retriever, let's configure a chat prompt template to pass to our chat model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    # Models like ChatAnthropic and Gemini-Pro do not accept \"system\" messages,\n",
    "    # otherwise I recommend using a system message to better steer the AI\n",
    "    (\"user\", \"You are an AI programming assistant and answer synthesist.\"),\n",
    "    (\"assistant\", \"I will do my best to answer accurately, honestly, and in pertinence to your context.\"),\n",
    "    (\"human\", \"\"\"[Task]: Answer the user's question based on the provided context, and only that context. If the context is not sufficient to answer the question, then say \"I don't know.\" \n",
    "        \n",
    "        [Context]:\n",
    "        {context}\n",
    "        \n",
    "        [User's question]:\n",
    "        {question}\n",
    "        \"\"\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we instantiate a Chat Model as an LLM to generate text. \n",
    "\n",
    "***Haiku** has been prioritized because it has a large context window, is cheap to use, and is far more capable than GPT-3.5-Turbo.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_anthropic import ChatAnthropic\n",
    "\n",
    "llm = ChatAnthropic(\n",
    "    model=\"claude-3-haiku-20240307\",\n",
    "    temperature=0.15,\n",
    "    streaming=True,\n",
    "    max_tokens=4096,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Invocation\n",
    "\n",
    "Below we define the chain we want to use. We provide the retriever's results as the context, which returns Documents based on the user's question. The context is stuffed into the `prompt_template` by way of [LCEL](https://python.langchain.com/docs/expression_language/ \"LCEL Docs\"), passed to the LLM where we generate output text for parsing and subsequently printing.\n",
    "\n",
    "The example invocation uses a coding example that requires context from the retriever in order to be correctly answered in full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is a list of Streamlit widgets and suggestions for widgets to use for the Streamlit Real-ESRGAN image upscaler:\n",
      "\n",
      "Streamlit Widgets:\n",
      "- st.button\n",
      "- st.download_button\n",
      "- st.link_button\n",
      "- st.page_link\n",
      "- st.checkbox\n",
      "- st.toggle\n",
      "- st.radio\n",
      "- st.selectbox\n",
      "- st.multiselect\n",
      "- st.slider\n",
      "- st.select_slider\n",
      "- st.text_input\n",
      "- st.number_input\n",
      "- st.text_area\n",
      "- st.date_input\n",
      "- st.time_input\n",
      "- st.file_uploader\n",
      "- st.camera_input\n",
      "- st.color_picker\n",
      "\n",
      "Suggested Widgets for Streamlit Real-ESRGAN:\n",
      "\n",
      "- `--input`: st.file_uploader or st.text_input (for folder path)\n",
      "- `--model_name`: st.selectbox\n",
      "- `--output`: st.text_input or st.directory_picker\n",
      "- `--denoise_strength`: st.slider\n",
      "- `--outscale`: st.slider\n",
      "- `--model_path`: st.text_input\n",
      "- `--suffix`: st.text_input\n",
      "- `--tile`: st.number_input\n",
      "- `--tile_pad`: st.number_input\n",
      "- `--pre_pad`: st.number_input\n",
      "- `--face_enhance`: st.checkbox\n",
      "- `--fp32`: st.checkbox\n",
      "- `--alpha_upsampler`: st.selectbox\n",
      "- `--ext`: st.selectbox\n",
      "- `--gpu-id`: st.number_input\n",
      "\n",
      "The key is to provide intuitive and user-friendly widgets that allow the user to easily configure the various parameters of the Real-ESRGAN image upscaler.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain = (\n",
    "    {\"context\": retriever}\n",
    "    | {\"question\": RunnablePassthrough()}\n",
    "    | prompt_template\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "invocation = chain.invoke(\n",
    "    \"\"\"\n",
    "List every single Streamlit widget. \n",
    "\n",
    "Then, suggest a widget side by side each of the following CLI arguments. Our job is to come up with widgets for the Streamlit Real-ESRGAN image upscaler:\n",
    "\n",
    "```python\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('-i', '--input', type=str, default='inputs', help='Input image or folder')\n",
    "    parser.add_argument(\n",
    "        '-n',\n",
    "        '--model_name',\n",
    "        type=str,\n",
    "        default='RealESRGAN_x4plus',\n",
    "        help=('Model names: RealESRGAN_x4plus | RealESRNet_x4plus | RealESRGAN_x4plus_anime_6B | RealESRGAN_x2plus | '\n",
    "              'realesr-animevideov3 | realesr-general-x4v3'))\n",
    "    parser.add_argument('-o', '--output', type=str, default='results', help='Output folder')\n",
    "    parser.add_argument(\n",
    "        '-dn',\n",
    "        '--denoise_strength',\n",
    "        type=float,\n",
    "        default=0.5,\n",
    "        help=('Denoise strength. 0 for weak denoise (keep noise), 1 for strong denoise ability. '\n",
    "              'Only used for the realesr-general-x4v3 model'))\n",
    "    parser.add_argument('-s', '--outscale', type=float, default=4, help='The final upsampling scale of the image')\n",
    "    parser.add_argument(\n",
    "        '--model_path', type=str, default=None, help='[Option] Model path. Usually, you do not need to specify it')\n",
    "    parser.add_argument('--suffix', type=str, default='out', help='Suffix of the restored image')\n",
    "    parser.add_argument('-t', '--tile', type=int, default=0, help='Tile size, 0 for no tile during testing')\n",
    "    parser.add_argument('--tile_pad', type=int, default=10, help='Tile padding')\n",
    "    parser.add_argument('--pre_pad', type=int, default=0, help='Pre padding size at each border')\n",
    "    parser.add_argument('--face_enhance', action='store_true', help='Use GFPGAN to enhance face')\n",
    "    parser.add_argument(\n",
    "        '--fp32', action='store_true', help='Use fp32 precision during inference. Default: fp16 (half precision).')\n",
    "    parser.add_argument(\n",
    "        '--alpha_upsampler',\n",
    "        type=str,\n",
    "        default='realesrgan',\n",
    "        help='The upsampler for the alpha channels. Options: realesrgan | bicubic')\n",
    "    parser.add_argument(\n",
    "        '--ext',\n",
    "        type=str,\n",
    "        default='auto',\n",
    "        help='Image extension. Options: auto | jpg | png, auto means using the same extension as inputs')\n",
    "    parser.add_argument(\n",
    "        '-g', '--gpu-id', type=int, default=None, help='gpu device to use (default=None) can be 0,1,2 for multi-gpu')\n",
    "```\n",
    "\"\"\"\n",
    ")\n",
    "print(invocation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
